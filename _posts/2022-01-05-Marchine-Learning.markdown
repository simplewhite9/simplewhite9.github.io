---
title:  "Machine Learningì˜ ê¸°ì´ˆ"
date:   2022-01-05 00:12:24 +0900
categories: [Deep Learning]
tags : [Supervised Learning, Overfitting, Underfitting, MSE, Regularization]
thumbnail-url: /assets/img/thumbnail/Deep-Learning-logo.png
description: this is a simple description of the post. this is a simple description of the post. this is a simple description of the post. 

---

ë”¥ëŸ¬ë‹ì˜ ì´í•´ë¥¼ ë” ë•ê¸° ìœ„í•´ ë¨¸ì‹  ëŸ¬ë‹ì˜ ê¸°ì´ˆë¥¼ ì§šê³  ë„˜ì–´ê°€ë ¤ê³  í•œë‹¤.

<div class="post-subtitle">Basic Mathematical Notation</div>

{% include callout-head.html %}âš ï¸{% include callout-body.html %}
<span class="bold">NOTE</span><br><br>

$xâˆˆ\mathbb{R}$ :  $x$ëŠ” <element class="bold highlight-red">scalar</element> <br>
$x\in\mathbb{R}^n$ : $x$ ëŠ” <span class="bold highlight-red">column</span> vector <br>
$x = (x_1, ...., x_n)$ë„  <span class="bold highlight-red">column</span> vector <br>

<span class="bold highlight-red">p-norm</span>: $||x||_p = (\sum_{i}^{}|x_i|^p)^\frac{1}{p})$ , $p \geq 1$
{% include callout-bottom.html %}
<br>


<div class="post-subtitle">Supervised Learning</div>

ì´ì „ì— ê¸€ì—ì„œ ì–¸ê¸‰ í–ˆë“¯ì´ supervised learningì€ ì •ë‹µê°’ì´ ìˆëŠ” (labeled) ë°ì´í„°ì…‹ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ê²ƒìœ¼ë¡œ classification (ë¶„ë¥˜)ì™€ regression(íšŒê·€)ë¡œ ë‚˜ë‰œë‹¤.

<img src="/assets/img/post1/post1-01.png" alt="Figure 1" width="300"/>
<div class="image-caption">Figure 1: Supervised classification example</div>



ìœ„ Figure 1ì€  **`classification`**ì˜ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ìœ ë°©ì•” í™˜ìë“¤ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì–‘ì„± ë˜ëŠ” ì•…ì„± ì¢…ì–‘ì˜ í™˜ìë¡œ êµ¬ë¶„ì„ í•˜ê³  ìˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” tumor sizeì™€ ë‚˜ì´ë¥¼ ê°€ì§€ê³  ì–‘ì„±ì´ë‹¤ ë˜ëŠ” ì•…ì„± ì¢…ì–‘ì´ë‹¤ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì´ì™€ ê°™ì´ ì˜ˆì¸¡ ê°’ì´ ì—°ì†ì ì´ì§€ ì•Šê³  ì •í™•íˆ 0 ë˜ëŠ” 1 ì•„ë‹ˆë©´ yes or no ë“±ìœ¼ë¡œ ë‚˜ì˜¬ ë•ŒëŠ” **classification**ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

**`regression`**ì€ ë°˜ëŒ€ë¡œ ì—°ì†ì ì¸ ì‹¤ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 

<img src="/assets/img/post1/post1-02.png" alt="Figure 2" width=650/>
<div class="image-caption">Figure 2: Supervised regression example</div>


ê° í™ë³´ ë§¤ì²´ TV, Radio, Newspaperì— ëŒ€í•œ í™ë³´ë¹„ìš© ê°’ì´ ì£¼ì–´ì¡Œì„ë•Œ ê° ë‘ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì¶” ì„¸ì„ ì„ ê·¸ì–´ì„œ ì˜ˆì¸¡í•˜ëŠ”ë° í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤.

Supervised learningì˜ processë¥¼ í•œëˆˆì— ë³´ë©´,

<img src="/assets/img/post1/post1-03.png" alt="Figure 3" width="300"/>
<div class="image-caption">Figure 3: Supervised learning process</div>

training dataë¡œ input $X$ ì™€ output $Y$ê°€ ì£¼ì–´ì¡Œì„ë•Œ ì˜ˆì¸¡ í•¨ìˆ˜ $h(X)$ ë¥¼ í†µí•´ $y$ ê°’ì„ ì¶”ì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.

{% include callout-head.html %}âš ï¸{% include callout-body.html %}
<span class="bold">NOTE</span><br><br>

$X$: input, <element class="bold">feature</element>, independent variable<br>
$Y$: <element class="bold">output</element>, response, dependent variable
{% include callout-bottom.html %}

<br>
ê¸°ë³¸ì ìœ¼ë¡œ $h(X)$ í•¨ìˆ˜ë¥¼ ë³´ë©´,
{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
<span class="bold">$Y = h(X) + \epsilon$</span><br><br>

$X$: feature vector $X = (X_1, X_2, ..., X_p)$ <br>
$h$ : unknown function <br>
$\epsilon$ : random error

{% include callout-bottom.html %}

ì´ë ‡ê²Œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì£¼ìš” ëª©í‘œëŠ” unknown function $h$ë¥¼ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.

<br>

---
<div class="post-subtitle">Function estimation</div>

1. **Population** (ëª¨ì§‘ë‹¨) : ì „ì²´ ì§‘í•©
2. **Sample** (í‘œë³¸): ëª¨ì§‘ë‹¨ì—ì„œì˜ ë¶€ë¶„ì§‘í•©

í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš© ë˜ëŠ” dataëŠ” populationì˜ ë¶€ë¶„ì§‘í•©ì¸ sampleì„ ì´ìš©í•œë‹¤. ì—¬ê¸°ì„œ sampleì€ populationì˜ **íŠ¹ì„±ì„ 99% ê°€ì§€ê³  ìˆë‹¤**ê³  ì¶”ì •ì„ í•˜ê³  ì§„í–‰í•˜ê³  1%ì˜ ì˜¤ì°¨ëŠ” í—ˆìš©ì´ ëœë‹¤.  

ì´ ë‘˜ì˜ ê´€ê³„ë¥¼ ê·¸ë¦¼ìœ¼ë¡œë³´ë©´, ì•„ë˜ì™€ ê°™ë‹¤.

<img src="/assets/img/post1/post1-04.png" alt="Figure 4" width="500"/>
<div class="image-caption">Figure 4: Population and Sample relationship</div>

<br>
ì´ì™€ ê°™ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì§€ê³ ,

<span class="background-yellow">$Y \approx \hat{h}(x)$</span>ë¥¼ ë§Œì¡±í•˜ëŠ” $h(x)$ì™€ ê°™ì´ ì‘ë™í•œë‹¤ê³  ì˜ˆì¸¡í•˜ëŠ” $\hat{h}(x)$ì˜ í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.

supervised learningì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •ë‹µì— ê°€ê¹Œìš´ í•¨ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ í‘œí˜„ë  ê²ƒ ì¸ê°€ì˜ ê°€ì„¤ì„ ë¨¼ì € ì„¸ì›Œì•¼í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” linear modelì„ íƒí•œë‹¤.

<br>
<div class="post-subtitle">Linear model:</div>

{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
<span class="bold">$\hat{h}(X) = \theta_0 + \theta_1x_1 + \theta_2{x_2} + ... + \theta_p{x_p}$</span><br><br>

$X = (x_1, x_2, x_3, ..., x_p)$ <element class="bold">feature</element> <br>
$\theta =$   <element class="bold">parameters</element> ( also known as <element class="bold">weights</element>) <br>
<span class="highlight-red">*NOTE $\theta_0$ëŠ” <element class="bold">bias</element>ë¼ ë¶€ë¥¸ë‹¤ <br></span>

{% include callout-bottom.html %}


{% include callout-head.html %}â•{% include callout-body.html %}
<span class="bold highlight-red">* $x_0 = 1$ì´ë¼ ê°€ì •í• ë•Œ,</span><br><br>

$\hat{h}(X) = \sum_{i = 0}^{p} \theta_ix_i = \theta^Tx$  ì´ë¼ê³ ë„ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤
{% include callout-bottom.html %}


{% include callout-head.html %}â•{% include callout-body.html %}
<span class="bold highlight-red">ì°¸ê³  ì‚¬í•­</span><br><br>

$\hat{h}(X) = \theta_0 + \theta_1x_1 + \theta_2{x_2} + ... + \theta_p{x_p}$ ì¼ë•Œ, <br>

$X = (x_0, x_1, x_2, x_3, ..., x_p)$ <br>
$\theta =$   $( \theta_1, \theta_2, \theta_3, ..., \theta_p)$ì™€ ê°™ì´ $\theta_0$ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì„ë•Œ, <br>
<br>
<span class="bold">$\hat{h}(X) = \tilde{\theta}^TX$</span> ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤ ì´ë•Œ $\tilde{\theta}$ëŠ” $( 1, \theta) = (1, \theta_1, \theta_2, ...)$ ì™€ ê°™ì€ ì˜ë¯¸ë‹¤
{% include callout-bottom.html %}
<br>

ì´ ì™¸ì— quadratic model ë„ ìˆë‹¤
<div class="post-subtitle">Quadratic model:</div>

{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
 $\hat{h}(X) = \theta_0 + \theta_1x_1 + \theta_2{x_2}^2 + ... + \theta_p{x_p}^p$

{% include callout-bottom.html %}

<span class="highlight-red">*Note:parameter ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ **flexibility ì„±ëŠ¥ì€ ì¦ê°€ë˜ë‚˜ generalization (ì¼ë°˜í™”) ì„±ëŠ¥ì€ ê°ì†Œëœë‹¤**</span>

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” **â€œ`data fitting`â€**ì˜ ì˜ë¯¸ëŠ” ìœ„ ì‹ì—ì„œ Y(ì •ë‹µ)ì— ê°€ì¥ ê·¼ì ‘í•œ ê°’ì„ ì–»ê¸° ìœ„í•´ í•¨ìˆ˜ì— í™œìš©ë˜ëŠ”  $p + 1$ê°œì˜ 
**<span class="background-yellow">parameters $\theta_i$ ë“¤ì˜ best approximation ê°’</span>**ì„ ì°¾ëŠ” ì‘ì—…ì´ë‹¤.

ê·¸ë ‡ë‹¤ë©´, í•¨ìˆ˜ $h$ë¥¼ **<span class="underline">ì˜ ì˜ˆì¸¡í•˜ëŠ”ê²Œ ì¤‘ìš”í•œ ì´ìœ </span>**!

1. ìƒˆë¡œìš´ í¬ì¸íŠ¸ $X = x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $Y$ê°’ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ 
2. ì£¼ì–´ì§„ feature ì¤‘ì—ì„œ ì–´ë–¤ feature ê°’ì´ ì¤‘ìš”í•œì§€ ì°¾ì„ ìˆ˜ ìˆë‹¤. ë‹¤ì‹œ ë§í•˜ë©´, $X = ( X_1, X_2, ..., X_p)$ë¼ê³  í• ë•Œ ì´ X ê°’ë“¤ ì¤‘ ì–´ë–¤ ìš”ì†Œê°€ Yë¥¼ ì„¤ëª…í•˜ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.
3. $h(X)$í•¨ìˆ˜ì˜ ë³µì¡ë„ì— ë”°ë¼ (ì°¨ìˆ˜ì™€ ì—°ê´€ë˜ì–´ ìˆë‹¤) ê° Xì˜ ìš”ì†Œ $X_j$ ë“¤ì´ Yì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤

<br>

---

ì•„ë˜ì™€ ê°™ì´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ 3ê°€ì§€ì˜ fittingì„ ì‹œë„í•´ë³´ì•˜ë‹¤ê³  í•˜ì,
<br>
<img src="/assets/img/post1/post1-05.png" alt="Figure 5" width="580"/>
<div class="image-caption">Figure 5: Results of fitting linear regression, quadratic regression and polynomial of 5-th order</div>


ë§¨ ì™¼ìª½ì€ linear fitting ($y = \theta_0+\theta_1x$), <br>
ë‘ë²ˆì§¸ëŠ” quadratic fitting ($y = \theta_0+\theta_1x + \theta_2x^2$), <br>
ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì€ 5-th order polynomial $y = \sum_{j = 0}^{5} \theta_jx^j$  ìœ¼ë¡œ fití•´ë³¸ ê²°ê³¼ì´ë‹¤. 

ì‚¬ì‹¤ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ë©´ì„œ ì ì  ë” ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.  ê·¸ëŸ¬ë‚˜ ê¼­ ê·¸ë ‡ì§€ë§Œì€ ì•Šë‹¤. ì´ë ‡ê²Œ ìƒˆë¡œìš´ feature $x^p$ë¥¼ ë„ì…ì‹œí‚¬ë•Œì—ëŠ” ì£¼ì˜í•´ì•¼í•˜ëŠ” ì‚¬í•­ë“¤ì´ ìˆë‹¤. 

<br>
<div class="post-subtitle"><span class="background-yellow">Overfitting</span></div>

ë§¨ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ overfittingì˜ ì˜ˆì‹œì´ë‹¤.

**Training data í•œì—ì„œëŠ” ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤**.

ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´, flexiblityê°€ ì¦ê°€í•˜ë©´ì„œ generalizationì´ ê°ì†Œí•˜ì—¬ ê³¼ì í•©ì˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê²ƒì´ë‹¤

ë³´í†µì€ validation lossì™€ training lossì˜ ìˆ˜ì¹˜ë¥¼ ë¹„êµí•˜ë©´ì„œ overfittingì´ ì¼ì–´ë‚¬ëŠ”ì§€ì˜ ì—¬ë¶€ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. ëª¨ë¸ Training ê³¼ì •ì—ì„œ ì–´ëŠìˆœê°„ training lossê°€ validation loss ë³´ë‹¤ í›¨ì”¬ ì‘ì•„ì§ˆë•Œ overfittingì´ ì¼ì–´ë‚˜ê³  ìˆìŒì„ ì˜ì‹¬í•  ìˆ˜ ìˆë‹¤. 

<br>
<div class="post-subtitle"><span class="background-yellow">Underfitting:</span></div> 

ë§¨ ì™¼ìª½ ê·¸ë¦¼ì€ underfittingì˜ ì˜ˆì‹œì´ë‹¤.

Overfittingê³¼ ë°˜ëŒ€ë˜ëŠ” ê°œë…ìœ¼ë¡œ, **Training dataë„ í•™ìŠµì„ ì œëŒ€ë¡œ í•˜ì§€ ëª»í•´ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ìƒíƒœ**ë¥¼ ë§í•œë‹¤.

<br>
ê²°êµ­ì—”, prediction accuracy ì™€ interpretabilityì˜ ì‹¸ì›€ì´ë‹¤ <br>
ëª¨ë¸ì´ ë‹¨ìˆœí• ìˆ˜ë¡ ë°ì´í„° í•´ì„ì€ ì‰¬ì›Œì§€ì§€ë§Œ, ê·¸ë§Œí¼ ì˜ˆì¸¡ ì •í™•ë„ëŠ” ë–¨ì–´ì§„ë‹¤.<br>
<br>

---

<div class="post-subtitle">ëª¨ë¸ì˜ ì •í™•ë„ ì¸¡ì •</div>

í”íˆ ì•Œë ¤ì ¸ ìˆëŠ” ë°©ë²•ìœ¼ë¡œëŠ” MSE (Mean Square Error)ê°€ ìˆë‹¤

<br>

**`Mean Square Error`:**

{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
$MSE = {1 \over N} \sum{(y- \hat{f}(x))^2}$ <br><br>

<span class="highlight-red">
<span class="bold">*ì˜¤ì°¨ = (ì‹¤ì œ ê°’ - ì˜ˆì¸¡ ê°’ )</span> <br>
ì˜¤ì°¨ëŠ” ì–‘ìˆ˜, ìŒìˆ˜ ë‹¤ì–‘í•˜ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í•´ë‹¹ ê°’ì„ ì œê³±í•œë‹¤  <br>
* ì œê³±í•˜ê¸° ë•Œë¬¸ì— <span class="bold">(ì˜ˆì¸¡ ê°’ - ì‹¤ì œ ê°’)</span>ìœ¼ë¡œë„ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤
</span>
{% include callout-bottom.html %}

í˜„ì¬ cost functionìœ¼ë¡œ MSEë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•˜ë©´ training ë‹¨ê³„ì—ì„œ optimizeí•˜ê³ ìí•˜ëŠ” **objective function**ì´ ë°”ë¡œ MSEê°€ ëœë‹¤.
<br>
ì¦‰, ëª¨ë¸ì„ training í•˜ë©´ì„œ MSE = 0ì´ ë˜ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.
- $MSE_{train} = {1 \over N} \sum_{i \in D_{train}}{(y_i- \hat{f}(x_i))^2}$

<br>

ë‹¤ë§Œ, training ë‹¨ê³„ì—ì„œ ë‹¨ìˆœíˆ $MSE_{train}$ ë§Œ **<span class="highlight-red">minimize</span>** í•˜ë ¤ê³  í•˜ë‹¤ë³´ë©´ overfittingì´ ì¼ì–´ë‚  ìˆ˜ë„ ìˆë‹¤.<br>
ê·¸ë˜ì„œ, í•´ë‹¹ ëª¨ë¸ì„ **independent(ë…ë¦½ëœ)** **test** datasetì„ í™œìš©í•´ì„œ validateì„ í•œë‹¤<br>
- $MSE_{test} = {1 \over N} \sum_{i \in D_{test}}{(y_i- \hat{f}(x_i))^2}$

<br>
**Example)**
<img src="/assets/img/post1/post1-06.png" alt="Figure 6" width="580"/>
<div class="image-caption">Figure 6.1: Comparison of curves with different flexibility</div>

ì™¼ìª½ì—ì„œ ê²€ì€ìƒ‰ ì„ ì´ ì •ë‹µì´ê³ , ì˜¤ë¥¸ìª½ ë¹¨ê°„ìƒ‰ ì„ ì€ $MSE_{test}$ ì´ê³  íšŒìƒ‰ ì„ ì€ $MSE_{train}$ì„ ë‚˜íƒ€ë‚¸ë‹¤. FlexibilityëŠ” parameterì˜ ê°œìˆ˜ì™€ ê°™ì€ ì˜ë¯¸ì¼ ë•Œ ë…¸ë‘ìƒ‰ ì„ ì€ flexibilityê°€ ë‚®ìŒìœ¼ë¡œ ì˜¤ì°¨ì˜ ë²”ìœ„ê°€ í¬ë‹¤. ë°˜ëŒ€ë¡œ ì´ˆë¡ìƒ‰ ì„ ì€ flexibilityê°€ ë†’ì•„ì„œ ì˜¤ì°¨ì˜ ë²”ìœ„ëŠ” ë…¸ë€ìƒ‰ë³´ë‹¤ ì‘ì§€ë§Œ trainig dataì—ì„œ ë‚˜ì˜¨ MSEê°’ì´ test dataì—ì„œ ë‚˜ì˜¨ ê°’ë³´ë‹¤ í˜„ì €íˆ ì‘ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì€ overfitting(ê³¼ì í•©)ì´ ëœ ëª¨ìŠµì´ë‹¤. ê·¸ëŸ¬ë©´ ì´ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•˜ê²Œ fittingëœ ëª¨ë¸ì€ íŒŒë€ìƒ‰ ì„ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 

<br>
<img src="/assets/img/post1/post1-07.png" alt="Figure 6" width="580"/>
<div class="image-caption">Figure 6.2: Comparison of curves with different flexibility ( but with less noise)</div>

ì´ë²ˆì—ëŠ” ìœ„ì— ê·¸ë¦¼ì„ ë³´ë©´, ë°ì´í„° ìì²´ê°€ ì•ì „ Figure 6.1ì— ë¹„í•´ì„œ noiseê°€ í›¨ì”¬ ì ë‹¤. ê²€ì€ìƒ‰ truth curveë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ëœ»ì´ë‹¤.<br>
ì´ëŸ° ê²½ìš°, ëª¨ë¸ ìì²´ëŠ” ë³µì¡í•œë° noiseê°€ ì ê¸° ë•Œë¬¸ì— flexibility ê°€ í›¨ì”¬ ë†’ì„ ë•Œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.<br>
ì¦‰, íŒŒë€ìƒ‰ ë˜ëŠ” ì´ˆë¡ìƒ‰ ì„ ì´ ì í•©í•˜ë‹¤ ë³¼ ìˆ˜ ìˆë‹¤.<br>


<br><br>
ìœ„ì—ì„œ ë³¸ **notationì„ ì¡°ê¸ˆ ë°”ê¿”ì„œ** MSE($L(X)$ **`loss function`**)ì„ ë‚˜íƒœë‚´ë©´,

{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
$L(X) = {1 \over N} \sum_{i = 1}^{N}{(\hat{f}(a_i;  x) - b_i)^2}$
<br><br>
$a = (a_1, a_2, a_3,..., a_m)$ <element class="bold">feature</element> <br>
$\hat{f}(a_i; x)$ = parameter $x$ë¥¼ ê°€ì§€ê³  $a_i$ë¥¼ í‘œí˜„í•œë‹¤ëŠ” ëœ»  $= ( \tilde{a_i}^Tx)$ <br>
$b =$  <element class="bold">output</element> ì‹¤ì œ ê°’ 
{% include callout-bottom.html %}


{% include callout-head.html %}â•{% include callout-body.html %}
 í˜¹ì€ $L(X) = {1 \over N} ||Ax - b||_2^2$ <br><br>

$A = [ \tilde{a_1}^T; \tilde{a_2}^T; \tilde{a_3}^T; , ...; \tilde{a_m}^T]$
{% include callout-bottom.html %}

 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œ **lossë¥¼ minimize**í•œë‹¤ëŠ” ê²ƒì€ <span class="highlight-red">$L(X)$**ì—ì„œì˜ ì˜¤ì°¨ ê±°ë¦¬ë¥¼ ìµœì†Œí™”**</span>í•˜ê³ ìí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.

<br>

---

<div class="post-subtitle">Overfitting ë°©ì§€ ë°©ë²•</div>

1. **`Regularization`** (ì •ê·œí™”)
2. Cross Validation (êµì°¨ ê²€ì¦)
3. ë” ë§ì€ ë°ì´í„° ì…ë ¥í•˜ê¸°

<br>
**`Regularization`**:

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” regularizationì— ëŒ€í•´ì„œë§Œ ë‹¤ë£° ê²ƒì´ë‹¤. Regularizationì€ overfittingì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì£¼ì–´ì§„ training dataì™€ noiseë¥¼ ì™¸ìš°ëŠ”(memorizing) í˜„ìƒì„ ë°©ì§€í•˜ëŠ” ê²ƒì´ë‹¤.

ì£¼ ëª©ì ì€ complexityë¥¼ ì¤„ì´ë©´ì„œ, generalization abilityëŠ” ë†’ì´ëŠ” ê²ƒì´ê³ , í•´ë‹¹ ë°©ë²•ìœ¼ë¡œëŠ” **parameter(weight)**ì˜ í¬ê¸°ë¥¼ ì‘ê²Œí•˜ëŠ” ê²ƒì´ ìˆë‹¤. 

<br>
ì¦‰, ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ <span class="highlight-gray">(L2 regularizationì˜ ì˜ˆì‹œì´ë‹¤)</span> <br>
(1) minimize $\|\|Ax - b\|\|_2^2$ í•˜ë©´ì„œ ë™ì‹œì— <br>
(2) minimize $\|\|x\|\|_2^2$ ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤ (magnitude of coefficients)

<span class="highlight-red">* NOTE: (1), (2)ëŠ” ë‘˜ë‹¤ **convex** **í•¨ìˆ˜**ì„ìœ¼ë¡œ min ë˜ëŠ” maxë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤</span>
<br><br>

ì´ì™€ê°™ì´ ë‘ê°œë¥¼ ë™ì‹œì— optimizeí•˜ëŠ” ê²ƒì„ ***multi-objective optimization*** ì´ë¼ê³  í•œë‹¤. ê²°êµ­ì—”,

{% include callout-head.html %}ğŸ“Œ{% include callout-body.html %}
 minimize $||Ax - b||_2^2$ + $\lambda ||x||_2^2$
<br><br>
$\lambda$ ëŠ” hyperparameterë¡œ ëª¨ë¸ì„ ìµœì†Œí™”í•˜ê±°ë‚˜ ì¡°ìœ¨í• ë•Œ ì“°ëŠ” ë³€ìˆ˜ì´ë‹¤
{% include callout-bottom.html %}

<br>
<img src="/assets/img/post1/post1-08.png" alt="Figure 7" width="580"/>
<div class="image-caption">Figure 7: Ridge regression; Regression depending on the variation of  lambda</div>
ìœ„ì™€ ê°™ì´ $\lambda$ì˜ ê°’ì— ë”°ë¼ regressionì´ ë°”ë€ŒëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

<br>
L1 regularizationì˜ ì˜ˆì‹œë¥¼ ë³´ë©´,<br>
minimize $\|\|Ax - b\|\|_2^2$ + $\lambda \|\|x\|\|_1$ìœ¼ë¡œ, $x$ê°€ 0 ì´ ë§ì€ ë²¡í„° **`sparse vector`**ë¥¼ ë§Œë“¤ê³ ì í• ë•Œ ì‚¬ìš©í•œë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ $x = (0, 0,..., 0, 1, 0.5)$.<br>
ì´ì™€ ê°™ì€ê±¸ **`Lasso regression`**ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤<br>


<br><br>

---

**ì°¸ê³ **:<br>
ê³ ë ¤ëŒ€í•™êµ COSE474 ë”¥ëŸ¬ë‹ <br>
Stanford University CS230 Deep Learning <br>
Stanford University CS229 Machine Learning <br>