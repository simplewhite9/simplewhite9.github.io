---
title:  "LESSON 2"
categories: CS224N
tags : [SGD, Negative Sampling]
thumbnail-url: /assets/img/thumbnail/stanford-nlp-logo-160.jpg
description: Neural Classification (Winter 2021 강의 정리)

---
# Neural Classification

---

<br>
<div class="post-subtitle">Bag of words model</div>

단어의 순서와 상관없이 빈도수를 통해 확률 값을 계산하는 모델이다.

즉, center word와 얼마나 가까운지 멀리 떨어져있는지는 고려하지 않는 것이다. 
예시가 바로 저번 시간에 다루었던 outside word vector와 center word vector의 dot product를 구하고 element wise softmax를 구하는 연산 과정이다. 
<br>
그러나 사실 NLP 에서는 조금 더 나아가 center word context안에 존재하는 단어들은 비교적 높은 확률 값이 나오기를 바랬다.

<br>


<div class="post-subtitle background-yellow">Stochastic Gradient Descent (확률적 경사 하강법)</div>

모델 학습을 잘 하기 위해서는 cost function $J(\theta)$ 를 minimize하는데 확률적 경사하강법을 활용한다.<br><br>

**Gradient descent vs Stochastic Gradient Descent:**

이 둘의 차이점은 간단하다. 우리가 사용하는 cost function $J(\theta)$는 주어진 corpus의 모든 windows를 고려하기 때문에 연산 비용이 높고 느리다. 그래서 차라리 하나의 center word 또는 batch of center word (batch: 묶음)를 선택해서 gradient를 계산하여 전체 기울기를 예측하는 것이다. 물론 noisy한 값을 제출하긴 하지만 연산 비용을 고려하면 효율적이다. 


{% include callout-head.html %}📌 {% include callout-body.html %}
 IDEA: $\theta^{new} = \theta^{old} \ - \ \alpha  \triangledown_{\theta}J(\theta)$ 

<br><br>
1. cost function $J(\theta)$의 <span class="bold">gradient</span> 구하기 <br>

2. <span class="bold highlight-red">negative gradient</span> 방향으로 <span class="bold highlight-red">small step $\alpha$ </span> 이동 <br>

3. 위를 반복 <br>
<br>
* $\alpha$: learning rate라고 부르기도 한다

{% include callout-bottom.html %}

<img src="/assets/img/CS224N/Lesson2/f1.png" alt="Figure 1" width="400"/>
<div class="image-caption">Figure1: Example of gradient descent algorithm</div>


<br>

---

<div class="post-subtitle">Negative sampling</div>

지금까지는 skip-gram model을 활용하는데 naive softmax으로 학습하는 방법을 배웠다. 

다시 기억을 해보자면, 

<img src="/assets/img/CS224N/Lesson2/f2.png" alt="Figure 1.5" width="330"/>

위와 같은 식을 계산하는 거였다. 그런데! 사실 이 분모 값을 계산할때 들어가는 연산 비용이 높다. 주어진 단어가 많으면 진행해야할 dot product도 증가하는 것이다.

그래서 softmax를 대신해서 binary regression logistic model를 학습하는 것인데 여기서 사용되는 데이터는 **(1)우리가 알고 있는 center word와 context window 안에 있는 단어들과** **(2)“noise” pairs라고 주어진 vocabulary에서 랜덤으로 선택한 단어**들이다.

<br>
이 학습 방법은 `logistic/sigmoid function`을 활용한다

{% include callout-head.html %}📌 {% include callout-body.html %}
Logistic/sigmoid function:

$\sigma(x) = { 1 \over 1 + e^{-x}}$, <br><br>
* range : (0, 1) real numbers

{% include callout-bottom.html %}

<img src="/assets/img/CS224N/Lesson2/f3.png" alt="Figure 2" width="370"/>
<div class="image-caption">Figure2: Graph of sigmoid function</div>

<br>
그렇다면 모델을 학습하는데 optimize 하고 싶은 $J(\theta)$는 앞전  ${ 1 \over T} \sum_{t - 1}^{T} J(\theta_t)$와 같다.

그러나 달라지는 것은 softmax가 사라지고 logistic function이 들어와서 식이 아래와 같다 (**maximize** 하고 싶다는 가정).

<img src="/assets/img/CS224N/Lesson2/f4.png" alt="Figure 4" width="420"/>

중요한 것은

(1)은 **true context 단어**들이 문맥상에 보여지는 확률이기 때문에 **maximize** 하고 싶고,

(2)은 **“noisy” pair** 단어들이 문맥상에 보여지는 확률이기 때문에 **minimize**를 하고 싶다.

그래서 뒷 부분만 minimize하기 위해서는 negative 붙여서 **negative** dot product를 계산하게 의도하면 sigmoid 함수에 넣었을때 훨씬 작은 값을 반환한다 (함수 그래프 참고). 

<br>

위 식을 minimize한다 하면 아래와 같다. 
<img src="/assets/img/CS224N/Lesson2/f5.jpeg" alt="Figure 5" width="460"/>

<br>

**k negative samples 선택하는 방법**:<br>
word probabilities를 활용하는데, 이는 $P(w) = {U(w)^{3/4} \cdot {1\over{Z}}}$ 이다.

$U(w)$ 는 unigram distribution인데 $3/4$승을 하는 이유는 빈도수가 낮은 단어들의 확률과 빈도수가 높은 단어들의 확률 차를 줄여주기 때문이다. 

<br>

그렇다면 **도대체 왜** <span class="highlight-red bold">co-occurrence count</span>를 가지고 어떤 단어가 무슨 단어와 자주 쓰이는지 통계를 내지 않는 이유는 뭘까?<br>
co-occurrence counts matrix를 만들 수 있는 방법은 두가지가 있다.

1. Window based<br>

    Example) corpus 안에 <br>
    - “I like deep Learning”
    - “I like NLP”
    - “I enjoy flying”<br><br>
    

    가 주어졌을 때,
    
    Window length 1 인 co-occurrence matrix은 아래와 같다. 
    <img src="/assets/img/CS224N/Lesson2/f6.png" alt="Figure 6" width="440"/>
    <div class="image-caption">Figure 3: Co-occurrence matrix</div>
    

1. full document<br>
    Document의 페이지, paragraph에 집중하여 가지고 co-occurence를 세는 것이다.
    

<br>
<span class="bold highlight-red">문제점:</span><br>
단어의 개수가 증가하면 할 수록 co-occurrence matrix의 크기도 같이 증가해 차지하는 공간이 크다. 심지어 데이터를 보면 `sparse`하다.

그리고 여러차례 실험을 하다가 사람들이 알아낸 것은 벡터가 low dimension일수록 더 좋은 결과를 반환할 수 있는 것이다. 

---
<br>
<div class="post-subtitle">Dimensionality Reduction</div>



앞에 언급 했던 것과 같이 더 좋은 성능을 내기 위해 주어진 벡터의 dimension을 최대한 줄이는 작업은 필수다. Singular Value Decomposition(SVD).

co-occurrence matrix X를 decompose 하는 방법은 아래와 같다.

<img src="/assets/img/CS224N/Lesson2/f7.png" alt="Figure 7" width="570"/>
<div class="image-caption">Figure 4: Example of Singular value decomposition (SVD)</div>



위와 같은 작업을 하고 나면 오르지 k개의 singular values만 남게 된다. <br>
<span class="highlight-gray">(왜 난 이 수학적인 부분이 기억이 안나는가..........)</span><br>
raw count에 하는 SVD 작업은 오히려 좋은 성적을 내지 못한다.

이유는 흔히 사용하는 “the”, “he”, “has”와 같은 단어들은 빈도수가 다른 단어들에 비해 훨씬 높기 때문이다.<br>
이 문제점을 해결하기 위해서는 빈도수에 log를 취하거나, min(X, t)를 하거나, 아예 function words를 무시해버리는 방법도 있다.

그러다가 2005년 co-occurrence model을 어떻게 더 발전시킬가? 하다가 나온 모델이 바로
COALS 모델이다.


<img src="/assets/img/CS224N/Lesson2/f8.png" alt="Figure 8" width="410"/>
<div class="image-caption">Figure 5: Example of COALS model</div>

이 모델에서 발견한 특이한 점은 transformation을 통해 semantic vector 패턴을 확인 할 수 있다. 그래서 알아낸 것이 linear components들이 서로 비슷한 벡터 크기와 방향을 가지고 있다는 것이다(초록색 화살표).

---

<br>

<div class="post-subtitle ">GloVe Model</div>

Count based 모델 (ex.COALS) 와 direct prediction (ex. Skip-gram)의 장점들을 통합한 모델이다.


<img src="/assets/img/CS224N/Lesson2/f9.png" alt="Figure 9" width="450"/>
<div class="image-caption">Figure 6: Count based model과 direct prediction들의 요소. Highlight된 빨간색은 GloVe에서 통합된 요소들을 의미</div>

<br>

<span class="bold highlight-red">중요</span>: GloVe 모델은 co-occurrence probabilities의 `ratio`를 활용한다

<img src="/assets/img/CS224N/Lesson2/f10.png" alt="Figure 10" width="430"/>
<div class="image-caption">Figure 7: Example of GloVe model co-occurrence probabilities
</div>


GloVe Model의 예시를 보면, ice가 center word 일때 solid가 나올 확률이 높다. 반대로 ice라는 단어의 문맥상에 gas가 나올 확률은 작다. 이를 토대로 마지막 열에 보면 ratio를 계산한 값을 볼 수 있다. 

ratio를 구하기 위해 필요한 식: 

- $w_i \cdot w_j = log P(i\|j)$ [Log-bilinear model]<br><br>
- $w_x \cdot (w_a - w_b) = log { P(x\|a) \over P(x\|b)}$
    
---

<br>

<div class="post-subtitle">Loss function</div>

<img src="/assets/img/CS224N/Lesson2/f11.png" alt="Figure 11" width="380"/>

$X_{ij}$ : co-occurence matrix

(1) f(x) ⇒ 빈도수가 높은 단어들의 지배를 막기 위해서 사용하는 함수

<img src="/assets/img/CS224N/Lesson2/f12.png" alt="Figure 12" width="240"/>


(2) $b_i + \hat{b_j}$ : bias

(3) 최대한 **difference**를 **minimize**하고 싶은 값

<br>

**Results**

frog 를 입력했을 때

1. frogs
2. toad
3. litoria
4. leptodactylidae

.... 

등 실제 개구리의 종류를 출력한다 (꽤나 좋은 성능을 보인다). 

---

<br>

<div class="post-subtitle">Word vector Evaluation</div>

Word vector의 성능이 얼마나 좋은지 평가하는 방법

1. Intrinsic (내부 평가) <br>
**specific/intermediate task**에 대한 평가.

    실제 정답이 주어져있어 해당 정답과 비교를 하는 것이기 때문에 word vector들이 얼마나 좋은지 바로 측정이 가능하다. 

    이 평가 방법은 계산이 빠르고 시스템을 이해하는데 도움을 주긴 하지만 해당 task를 optimize한다고 전체 시스템을 개선할 수 있는지는 불분명하다. 

    <img src="/assets/img/CS224N/Lesson2/f13.png" alt="Figure 13" width="310"/>
    <div class="image-caption">Figure 8: Example of intrinsic evaluation
    </div>



    예를 들어, man에 대응하는 단어가 woman이라고 한다면, king에 대응하는 단어는 무엇인지 물어봤을 때 queen이 나오도록 유도하는 것이다. 

    <img src="/assets/img/CS224N/Lesson2/f14.png" alt="Figure 14" width="450"/>

    사람이 직접 평가하는 것(human judgement)도 intrinsic 평가의 예시이기도 한다

    <img src="/assets/img/CS224N/Lesson2/f15.png" alt="Figure 15" width="310"/>
    <div class="image-caption">Figure 9: Human judgement correlation between words
    </div>


    단어와 단어간의 correlation 점수를 미리 평가하고 그 값을 모델에서 나오는 correlation coefficient 값과 비교한다 (이쪽 분야에서도 GloVe가 훨씬 좋은 결과를 가져왔다). <br><br>



2. Extrinsic (외부 평가)<br>
**real task**에 대한 평가.

    예를 들어 사람들이 관심있어 하는 web search 또는 machine translation 등을 개선하는데 집중하는 것이다. 그래서 NLP system과 비교했을 때 성능이 개선되었는지 안되었는지를 확인하면서 평가한다. <br>
    그러나 값을 측정하는데 더 오래 걸리고, subsystem들 간에 상호작용이 불분명하다.<br><br>

    Another common example: **named entity recognition**

    주어진 단어가 어떤 위치, 단체, 사람을 나타내는지 판별한다 
    (Korea University라고 적혀있다면 organization으로 구별)<br><br>
    

    Analogy evaluation and **`hyperparameters`**<br>
    GloVe 모델간 서로 다른 hyperparameter를 이용했을때 성능이 달라지는 것도 확인 할 수 있었다. 

    1. **데이터가 많을 수록** 좋다
    2. News text보다 **Wikipedia**를 활용하는 것이 더 좋은 성능을 보여줬다. 

    <br>
    차원수가 높아질수록 모델 성능은?<br>
    차원수가 거의 **300**일때 가장 좋은 결과를 보여 준다. <br><br>

---

<br>

<div class="post-subtitle ">Word senses (ambiguity) </div> 


문맥에 의해서 단어의 해석은 여러가지로 나뉠 수 있다. 특히나 흔히 사용되는 단어이거나, 오래전부터 사용 되는 단어일수록 뜻이 달라진다. 

이 수업에서 보여준 예시는 “**pike**”

1. 가시, 바늘
2. 강꼬치고기라는 물고기
3. 도로의 한 종류
4. 다이빙할때 사용되는 자세의 레이블 중 하나 
5. 사람을 찌르는 행위 ( verb )

...

등 여러가지의 의미를 가지고 있다. 
특히나 명사로도 사용이 될 수 있고 또는 동사로 사용이 될 수도 있다.<br>
<span class="highlight-gray">(내 인생에서 pike라는 단어에 이렇게 많은 뜻이 있는 줄 처음 알았다)</span>

<br>
그래서 이렇게 다양한 해석을 갖는 단어는 word vector로 나타낼때는 각 다른 의미에 따라 여러가지의 word vector로 나타낸다 (`word sense vectors`).

word vector들은 linear superposition을 갖는다 (linear 형태로 표현이 가능하다).

<img src="/assets/img/CS224N/Lesson2/f16.png" alt="Figure 16" width="340"/>


<img src="/assets/img/CS224N/Lesson2/f17.png" alt="Figure 17" width="300"/>


여기서 신기한 것은 average vector(ex.$v_{pike}$)은 결국엔 서로 다른 단어의 해석의 차이를 분명하게 보여준다(disambiguate 한다고 칭한다).<br>
차원수가 높아질수록 데이터 표현이 sparse해진다. 이런 특정을 살려, sparse coding을 통해 문맥상에서 어떤 의미로 쓰였는지 구별이 가능하다. <br>

*sparse coding: linear combination of basis vector

끝✌🏻

---

**참조:**

Stanford University CS224n (Winter 2021)<br>
고려대학교 COSE461  자연어처리